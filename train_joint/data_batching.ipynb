{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from pyntcloud import PyntCloud\n",
    "\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data_pcl_torch, batch_size = 20, max_point_num = 150000, grid_size=1000):\n",
    "    \n",
    "    data_num = np.zeros((batch_size), dtype=np.int32)\n",
    "    indices_split_to_full = np.zeros((batch_size, max_point_num), dtype=np.int32)\n",
    "    \n",
    "    data_pcl = data_pcl_torch.numpy()\n",
    "    print(data_pcl.shape)\n",
    "    \n",
    "    data_pcl_torch = data_pcl_torch.unsqueeze(0)\n",
    "    data_pcl_torch = data_pcl_torch.numpy()\n",
    "    \n",
    "    xyz =data_pcl[:,0:3]\n",
    "    i = data_pcl[:,3]\n",
    "    xyz_min = np.amin(xyz, axis=0, keepdims=True)\n",
    "    print(\"min \", xyz_min)\n",
    "    xyz_max = np.amax(xyz, axis=0, keepdims=True)\n",
    "    print(\"max \", xyz_max)\n",
    "    block_size = (2 * (xyz_max[0, 0] - xyz_min[0, 0]), 2 * (xyz_max[0, 1] - xyz_min[0, 1]) ,  2 * (xyz_max[0, -1] - xyz_min[0, -1]))\n",
    "    \n",
    "    xyz_blocks = np.floor((xyz - xyz_min) / block_size).astype(np.int)\n",
    "    print(\"block size \", block_size)\n",
    "    print(\"diff block \",(xyz - xyz_min))\n",
    "    #print('{}-Collecting points belong to each block...'.format(datetime.now(), xyzrcof.shape[0]))\n",
    "    print(\" blocks \",xyz_blocks)\n",
    "    print(np.unique(xyz_blocks, return_inverse=True,return_counts=True, axis=0))\n",
    "    blocks, point_block_indices, block_point_counts = np.unique(xyz_blocks, return_inverse=True,\n",
    "                                                                return_counts=True, axis=0)\n",
    "    block_point_indices = np.split(np.argsort(point_block_indices), np.cumsum(block_point_counts[:-1]))\n",
    "    #print('{}-{} is split into {} blocks.'.format(datetime.now(), dataset, blocks.shape[0]))\n",
    "\n",
    "    block_to_block_idx_map = dict()\n",
    "    for block_idx in range(blocks.shape[0]):\n",
    "        block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "        block_to_block_idx_map[(block[0], block[1])] = block_idx\n",
    "\n",
    "    # merge small blocks into one of their big neighbors\n",
    "    block_point_count_threshold = max_point_num / 3\n",
    "    print(\"threshold \", block_point_count_threshold)\n",
    "    #print(\"block_point_count_threshold\",block_point_count_threshold)\n",
    "    nbr_block_offsets = [(0, 1), (1, 0), (0, -1), (-1, 0), (-1, 1), (1, 1), (1, -1), (-1, -1)]\n",
    "    block_merge_count = 0\n",
    "    print(\"blocking\")\n",
    "    print(block_point_counts)\n",
    "    print(point_block_indices)\n",
    "    print(blocks)\n",
    "    for block_idx in range(blocks.shape[0]):\n",
    "        if block_point_counts[block_idx] >= block_point_count_threshold:\n",
    "            print(\"here\")\n",
    "            print(block_idx, block_point_counts[block_idx])\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "        for x, y in nbr_block_offsets:\n",
    "            nbr_block = (block[0] + x, block[1] + y)\n",
    "            if nbr_block not in block_to_block_idx_map:\n",
    "                continue\n",
    "\n",
    "            nbr_block_idx = block_to_block_idx_map[nbr_block]\n",
    "            if block_point_counts[nbr_block_idx] < block_point_count_threshold:\n",
    "                continue\n",
    "\n",
    "\n",
    "            #print(block_idx, nbr_block_idx, block_point_counts[nbr_block_idx])\n",
    "\n",
    "            block_point_indices[nbr_block_idx] = np.concatenate(\n",
    "                [block_point_indices[nbr_block_idx], block_point_indices[block_idx]], axis=-1)\n",
    "            block_point_indices[block_idx] = np.array([], dtype=np.int)\n",
    "            block_merge_count = block_merge_count + 1\n",
    "            break\n",
    "    #print('{}-{} of {} blocks are merged.'.format(datetime.now(), block_merge_count, blocks.shape[0]))\n",
    "\n",
    "    idx_last_non_empty_block = 0\n",
    "    for block_idx in reversed(range(blocks.shape[0])):\n",
    "        if block_point_indices[block_idx].shape[0] != 0:\n",
    "            idx_last_non_empty_block = block_idx\n",
    "            break\n",
    "\n",
    "    # uniformly sample each block\n",
    "    for block_idx in range(idx_last_non_empty_block + 1):\n",
    "        point_indices = block_point_indices[block_idx]\n",
    "        if point_indices.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        #print(block_idx, point_indices.shape)\n",
    "        block_points = xyz[point_indices]\n",
    "        block_min = np.amin(block_points, axis=0, keepdims=True)\n",
    "        xyz_grids = np.floor((block_points - block_min) / grid_size).astype(np.int)\n",
    "        grids, point_grid_indices, grid_point_counts = np.unique(xyz_grids, return_inverse=True,\n",
    "                                                                 return_counts=True, axis=0)\n",
    "        grid_point_indices = np.split(np.argsort(point_grid_indices), np.cumsum(grid_point_counts[:-1]))\n",
    "        grid_point_count_avg = int(np.average(grid_point_counts))\n",
    "        point_indices_repeated = []\n",
    "        for grid_idx in range(grids.shape[0]):\n",
    "            point_indices_in_block = grid_point_indices[grid_idx]\n",
    "            repeat_num = math.ceil(grid_point_count_avg / point_indices_in_block.shape[0])\n",
    "            if repeat_num > 1:\n",
    "                point_indices_in_block = np.repeat(point_indices_in_block, repeat_num)\n",
    "                np.random.shuffle(point_indices_in_block)\n",
    "                point_indices_in_block = point_indices_in_block[:grid_point_count_avg]\n",
    "            point_indices_repeated.extend(list(point_indices[point_indices_in_block]))\n",
    "        block_point_indices[block_idx] = np.array(point_indices_repeated)\n",
    "        block_point_counts[block_idx] = len(point_indices_repeated)\n",
    "\n",
    "    idx = 0\n",
    "    for block_idx in range(idx_last_non_empty_block + 1):\n",
    "        point_indices = block_point_indices[block_idx]\n",
    "        if point_indices.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        block_point_num = point_indices.shape[0]\n",
    "        block_split_num = int(math.ceil(block_point_num * 1.0 / max_point_num))\n",
    "        point_num_avg = int(math.ceil(block_point_num * 1.0 / block_split_num))\n",
    "        point_nums = [point_num_avg] * block_split_num\n",
    "        point_nums[-1] = block_point_num - (point_num_avg * (block_split_num - 1))\n",
    "        starts = [0] + list(np.cumsum(point_nums))\n",
    "\n",
    "        np.random.shuffle(point_indices)\n",
    "        block_points = xyz[point_indices]\n",
    "\n",
    "\n",
    "        block_min = np.amin(block_points, axis=0, keepdims=True)\n",
    "        block_max = np.amax(block_points, axis=0, keepdims=True)\n",
    "        #block_center = (block_min + block_max) / 2\n",
    "        #block_center[0][-1] = block_min[0][-1]\n",
    "        #block_points = block_points - block_center  # align to block bottom center\n",
    "        x, y, z = np.split(block_points, (1, 2), axis=-1)\n",
    "        print(i.shape)\n",
    "        print(x.shape)\n",
    "        block_xzyrgbi = np.concatenate([x, z, y, i[:, np.newaxis][point_indices]], axis=-1)\n",
    "        print(block_xzyrgbi.shape)\n",
    "        for block_split_idx in range(block_split_num):\n",
    "            start = starts[block_split_idx]\n",
    "            point_num = point_nums[block_split_idx]\n",
    "            #print(block_split_num, block_split_idx, point_num )\n",
    "\n",
    "\n",
    "\n",
    "            end = start + point_num\n",
    "            idx_in_batch = idx % batch_size\n",
    "            print(idx_in_batch)\n",
    "            data_pcl_torch[idx_in_batch, 0:point_num, ...] = block_xzyrgbi[start:end, :]\n",
    "            data_num[idx_in_batch] = point_num\n",
    "            indices_split_to_full[idx_in_batch, 0:point_num] = point_indices[start:end]\n",
    "\n",
    "            #print(\"indices_split_to_full\", idx_in_batch, point_num, indices_split_to_full)\n",
    "\n",
    "            if  (block_idx == idx_last_non_empty_block and block_split_idx == block_split_num - 1): #Last iteration\n",
    "\n",
    "                item_num = idx_in_batch + 1\n",
    "                \n",
    "            idx = idx + 1\n",
    "    return data_pcl_torch, data_num, indices_split_to_full, item_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file = np.fromfile(os.path.join(\"000000.bin\"),dtype=np.float32).reshape((-1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcl shape  torch.Size([1, 123397, 4])\n",
      "(123397, 4)\n",
      "min  [[-64.347  -6.962 -79.833]]\n",
      "max  [[68.529  2.897 77.739]]\n",
      "block size  (265.75201416015625, 19.718000411987305, 315.14398193359375)\n",
      "diff block  [[51.366      6.3919997 75.531    ]\n",
      " [31.949001   6.183     99.169    ]\n",
      " [76.429      5.858     83.012    ]\n",
      " ...\n",
      " [71.728      5.248     76.156    ]\n",
      " [59.96       5.048     85.912    ]\n",
      " [66.715      7.446     87.598    ]]\n",
      " blocks  [[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " ...\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "(array([[0, 0, 0]]), array([0, 0, 0, ..., 0, 0, 0]), array([123397]))\n",
      "threshold  50000.0\n",
      "blocking\n",
      "[123397]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[0 0 0]]\n",
      "here\n",
      "0 123397\n",
      "(123397,)\n",
      "(123397, 1)\n",
      "(123397, 4)\n",
      "0\n",
      "(1, 123397, 4)\n",
      "(20,)\n",
      "(20, 150000)\n",
      "1\n",
      "[[[ 14.874  14.839  -1.605   0.35 ]\n",
      "  [-20.778  -3.804  -0.991   0.42 ]\n",
      "  [  5.198  -3.159  -1.687   0.25 ]\n",
      "  ...\n",
      "  [  6.882   1.893  -1.737   0.17 ]\n",
      "  [  4.021  -3.309  -1.707   0.26 ]\n",
      "  [-31.75    7.92    0.156   0.25 ]]]\n",
      "(1, 123397, 4)\n",
      "(1,)\n",
      "(1, 150000)\n",
      "(20, 123397, 4)\n",
      "123397\n"
     ]
    }
   ],
   "source": [
    "main_tensor = np.arange(150000*4).reshape((150000,4))\n",
    "# main_tensor = load_file[:, np.newaxis]\n",
    "main_tensor = load_file\n",
    "batch_size = 20\n",
    "# main_tensor[:full_data.shape[0], :full_data.shape[1]] = full_data\n",
    "data_pcl = torchvision.transforms.ToTensor()(main_tensor)\n",
    "print(\"pcl shape \",data_pcl.shape)\n",
    "for each_batch in data_pcl:\n",
    "    batched_data, data_num, indices_split_to_full, item_num = get_batches(each_batch, batch_size = batch_size)\n",
    "    print(batched_data.shape)\n",
    "    print(data_num.shape)\n",
    "    print(indices_split_to_full.shape)\n",
    "    print(item_num)\n",
    "    print(batched_data)\n",
    "    \n",
    "    data =batched_data[0:item_num, ...].astype(np.float32) \n",
    "    print(data.shape)\n",
    "\n",
    "    data_num =data_num[0:item_num, ...] \n",
    "    print(data_num.shape)\n",
    "    indices_split_to_full = indices_split_to_full[0:item_num]\n",
    "    print(indices_split_to_full.shape)\n",
    "\n",
    "    batch_num = data.shape[0]\n",
    "    for batch_idx in range(batch_num):\n",
    "        points_batch = data[[batch_idx] * batch_size, ...]\n",
    "        print(points_batch.shape)\n",
    "        point_num = data_num[batch_idx]\n",
    "        print(point_num)\n",
    "        break\n",
    "\n",
    "        tile_num = int ( math.ceil((sample_num * batch_size) / point_num) )\n",
    "        indices_shuffle = np.tile(np.arange(point_num), tile_num)[0:sample_num * batch_size]\n",
    "        np.random.shuffle(indices_shuffle)\n",
    "        indices_batch_shuffle = np.reshape(indices_shuffle, (batch_size, sample_num, 1))\n",
    "        indices_batch = np.concatenate((indices_batch_indices, indices_batch_shuffle), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(max_point_num):\n",
    "    batch_size = 2048\n",
    "    block_size = 1000\n",
    "    grid_size = 0.25\n",
    "        \n",
    "    \n",
    "    data = np.zeros((batch_size, max_point_num, 4))\n",
    "    data_num = np.zeros((batch_size), dtype=np.int32)\n",
    "    label = np.zeros((batch_size), dtype=np.int32)\n",
    "    label_seg = np.zeros((batch_size, max_point_num), dtype=np.int32)\n",
    "    indices_split_to_full = np.zeros((batch_size, max_point_num), dtype=np.int32)\n",
    "\n",
    "\n",
    "    \n",
    "    xyzi = np.fromfile(\"000000.bin\",dtype=np.float32).reshape((-1, 4))\n",
    "    \n",
    "    \n",
    "    indices_for_prediction = np.arange(xyzi.shape[0]) #(xyzi[:,0] >= -5 ).nonzero()[0]\n",
    "    #print(\"indices_for_prediction\", indices_for_prediction)\n",
    "    # Filter point only in front on of ego-sensors\n",
    "    xyzif =xyzi #= xyzi[xyzi[:,0] >= -5 ] \n",
    "    \n",
    "    all_label_pred = np.zeros((xyzi.shape[0]),dtype=int)\n",
    "    label_length = xyzif.shape[0]\n",
    "    xyz =xyzif[:,0:3]\n",
    "    \n",
    "    i =( xyzif[:,3:4] / np.max(xyzif[:,3:4].flatten()) ) \n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "    xyz_min = np.amin(xyz, axis=0, keepdims=True)\n",
    "    xyz_max = np.amax(xyz, axis=0, keepdims=True)\n",
    "    block_size = (2 * (xyz_max[0, 0] - xyz_min[0, 0]), 2 * (xyz_max[0, 1] - xyz_min[0, 1]) ,  2 * (xyz_max[0, -1] - xyz_min[0, -1]))\n",
    "    \n",
    "    xyz_blocks = np.floor((xyz - xyz_min) / block_size).astype(np.int)\n",
    "\n",
    "    #print('{}-Collecting points belong to each block...'.format(datetime.now(), xyzrcof.shape[0]))\n",
    "    blocks, point_block_indices, block_point_counts = np.unique(xyz_blocks, return_inverse=True,\n",
    "                                                                return_counts=True, axis=0)\n",
    "    block_point_indices = np.split(np.argsort(point_block_indices), np.cumsum(block_point_counts[:-1]))\n",
    "    #print('{}-{} is split into {} blocks.'.format(datetime.now(), dataset, blocks.shape[0]))\n",
    "\n",
    "    block_to_block_idx_map = dict()\n",
    "    for block_idx in range(blocks.shape[0]):\n",
    "        block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "        block_to_block_idx_map[(block[0], block[1])] = block_idx\n",
    "\n",
    "    # merge small blocks into one of their big neighbors\n",
    "    block_point_count_threshold = max_point_num / 3\n",
    "    #print(\"block_point_count_threshold\",block_point_count_threshold)\n",
    "    nbr_block_offsets = [(0, 1), (1, 0), (0, -1), (-1, 0), (-1, 1), (1, 1), (1, -1), (-1, -1)]\n",
    "    block_merge_count = 0\n",
    "    for block_idx in range(blocks.shape[0]):\n",
    "        if block_point_counts[block_idx] >= block_point_count_threshold:\n",
    "            #print(block_idx, block_point_counts[block_idx])\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "        for x, y in nbr_block_offsets:\n",
    "            nbr_block = (block[0] + x, block[1] + y)\n",
    "            if nbr_block not in block_to_block_idx_map:\n",
    "                continue\n",
    "\n",
    "            nbr_block_idx = block_to_block_idx_map[nbr_block]\n",
    "            if block_point_counts[nbr_block_idx] < block_point_count_threshold:\n",
    "                continue\n",
    "\n",
    "\n",
    "            #print(block_idx, nbr_block_idx, block_point_counts[nbr_block_idx])\n",
    "\n",
    "            block_point_indices[nbr_block_idx] = np.concatenate(\n",
    "                [block_point_indices[nbr_block_idx], block_point_indices[block_idx]], axis=-1)\n",
    "            block_point_indices[block_idx] = np.array([], dtype=np.int)\n",
    "            block_merge_count = block_merge_count + 1\n",
    "            break\n",
    "    #print('{}-{} of {} blocks are merged.'.format(datetime.now(), block_merge_count, blocks.shape[0]))\n",
    "\n",
    "    idx_last_non_empty_block = 0\n",
    "    for block_idx in reversed(range(blocks.shape[0])):\n",
    "        if block_point_indices[block_idx].shape[0] != 0:\n",
    "            idx_last_non_empty_block = block_idx\n",
    "            break\n",
    "\n",
    "    # uniformly sample each block\n",
    "    for block_idx in range(idx_last_non_empty_block + 1):\n",
    "        point_indices = block_point_indices[block_idx]\n",
    "        if point_indices.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        #print(block_idx, point_indices.shape)\n",
    "        block_points = xyz[point_indices]\n",
    "        block_min = np.amin(block_points, axis=0, keepdims=True)\n",
    "        xyz_grids = np.floor((block_points - block_min) / grid_size).astype(np.int)\n",
    "        grids, point_grid_indices, grid_point_counts = np.unique(xyz_grids, return_inverse=True,\n",
    "                                                                 return_counts=True, axis=0)\n",
    "        grid_point_indices = np.split(np.argsort(point_grid_indices), np.cumsum(grid_point_counts[:-1]))\n",
    "        grid_point_count_avg = int(np.average(grid_point_counts))\n",
    "        point_indices_repeated = []\n",
    "        for grid_idx in range(grids.shape[0]):\n",
    "            point_indices_in_block = grid_point_indices[grid_idx]\n",
    "            repeat_num = math.ceil(grid_point_count_avg / point_indices_in_block.shape[0])\n",
    "            if repeat_num > 1:\n",
    "                point_indices_in_block = np.repeat(point_indices_in_block, repeat_num)\n",
    "                np.random.shuffle(point_indices_in_block)\n",
    "                point_indices_in_block = point_indices_in_block[:grid_point_count_avg]\n",
    "            point_indices_repeated.extend(list(point_indices[point_indices_in_block]))\n",
    "        block_point_indices[block_idx] = np.array(point_indices_repeated)\n",
    "        block_point_counts[block_idx] = len(point_indices_repeated)\n",
    "\n",
    "    idx = 0\n",
    "    for block_idx in range(idx_last_non_empty_block + 1):\n",
    "        point_indices = block_point_indices[block_idx]\n",
    "        if point_indices.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        block_point_num = point_indices.shape[0]\n",
    "        block_split_num = int(math.ceil(block_point_num * 1.0 / max_point_num))\n",
    "        point_num_avg = int(math.ceil(block_point_num * 1.0 / block_split_num))\n",
    "        point_nums = [point_num_avg] * block_split_num\n",
    "        point_nums[-1] = block_point_num - (point_num_avg * (block_split_num - 1))\n",
    "        starts = [0] + list(np.cumsum(point_nums))\n",
    "\n",
    "        np.random.shuffle(point_indices)\n",
    "        block_points = xyz[point_indices]\n",
    "\n",
    "\n",
    "        block_min = np.amin(block_points, axis=0, keepdims=True)\n",
    "        block_max = np.amax(block_points, axis=0, keepdims=True)\n",
    "        #block_center = (block_min + block_max) / 2\n",
    "        #block_center[0][-1] = block_min[0][-1]\n",
    "        #block_points = block_points - block_center  # align to block bottom center\n",
    "        x, y, z = np.split(block_points, (1, 2), axis=-1)\n",
    "\n",
    "        block_xzyrgbi = np.concatenate([x, z, y, i[point_indices]], axis=-1)\n",
    "\n",
    "        for block_split_idx in range(block_split_num):\n",
    "            start = starts[block_split_idx]\n",
    "            point_num = point_nums[block_split_idx]\n",
    "            #print(block_split_num, block_split_idx, point_num )\n",
    "\n",
    "\n",
    "\n",
    "            end = start + point_num\n",
    "            idx_in_batch = idx % batch_size\n",
    "            data[idx_in_batch, 0:point_num, ...] = block_xzyrgbi[start:end, :]\n",
    "            data_num[idx_in_batch] = point_num\n",
    "            indices_split_to_full[idx_in_batch, 0:point_num] = point_indices[start:end]\n",
    "\n",
    "            #print(\"indices_split_to_full\", idx_in_batch, point_num, indices_split_to_full)\n",
    "\n",
    "            if  (block_idx == idx_last_non_empty_block and block_split_idx == block_split_num - 1): #Last iteration\n",
    "\n",
    "                item_num = idx_in_batch + 1\n",
    "                \n",
    "            idx = idx + 1\n",
    "            \n",
    "    return label_length, data, data_num, indices_split_to_full, item_num, all_label_pred, indices_for_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1\n",
      " 123397  2\n",
      " [[[-30.64100075   0.52200001   5.66599989   0.14141414]\n",
      "  [ 34.38199997  -1.91700006  14.93900013   0.        ]\n",
      "  [ -1.40499997  -1.78100002   4.37599993   0.2020202 ]\n",
      "  ...\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[-11.67199993  -0.62800002  -8.92599964   0.32323232]\n",
      "  [ -4.28999996  -0.77899998 -12.59899998   0.37373737]\n",
      "  [-28.0170002   -1.58000004   6.73500013   0.        ]\n",
      "  ...\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  ...\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  ...\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  ...\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  ...\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]] [83993 83993     0 ...     0     0     0]  3\n",
      " [[  8444  28981 116233 ...      0      0      0]\n",
      " [ 30099  36495  31838 ...      0      0      0]\n",
      " [     0      0      0 ...      0      0      0]\n",
      " ...\n",
      " [     0      0      0 ...      0      0      0]\n",
      " [     0      0      0 ...      0      0      0]\n",
      " [     0      0      0 ...      0      0      0]] 4 \n",
      " 2 5 \n",
      " [0 0 0 ... 0 0 0]  6\n",
      " [     0      1      2 ... 123394 123395 123396]\n",
      "(2048, 150000, 4)\n",
      "dat  (2, 150000, 4)\n",
      "dat2  (2,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_point_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-45d31d5ace5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mlabels_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_point_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mconfidences_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_point_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_point_num' is not defined"
     ]
    }
   ],
   "source": [
    "label_length, data, data_num, indices_split_to_full, item_num, all_label_pred, indices_for_prediction = data_preprocessing(150000)\n",
    "print(\" 1\\n\" , label_length,\" 2\\n\", data, data_num,\" 3\\n\", indices_split_to_full,\"4 \\n\", item_num, \"5 \\n\",all_label_pred,\" 6\\n\", indices_for_prediction)\n",
    "print(data.shape)\n",
    "merged_label_zero = np.zeros((label_length),dtype=int)\n",
    "merged_confidence_zero = np.zeros((label_length),dtype=float)\n",
    "\n",
    "\n",
    "data =data[0:item_num, ...].astype(np.float32) \n",
    "print(\"dat \",data.shape)\n",
    "\n",
    "data_num =data_num[0:item_num, ...] \n",
    "print(\"dat2 \",data_num.shape)\n",
    "indices_split_to_full = indices_split_to_full[0:item_num]\n",
    "\n",
    "batch_num = data.shape[0]\n",
    "\n",
    "labels_pred = np.full((batch_num, max_point_num), -1, dtype=np.int32)\n",
    "confidences_pred = np.zeros((batch_num, max_point_num), dtype=np.float32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
