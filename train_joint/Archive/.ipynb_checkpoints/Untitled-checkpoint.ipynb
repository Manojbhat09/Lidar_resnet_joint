{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import glob\n",
    "\n",
    "from pyntcloud import PyntCloud\n",
    "\n",
    "# class ArgoverseDataset(Dataset):\n",
    "\n",
    "#     def __init__(self, data_dir, min_past_obv_len=2, min_future_obv_len=10, min_future_pred_len=15, transform=None, num_workers=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#         :param data : List of [scene_id, scene_image, number_agents, past_list, future_list,\n",
    "#                                encode_coordinates, decode_coordinates]\n",
    "#         \"\"\"\n",
    "#         self.transform = transform\n",
    "#         self.num_workers = num_workers\n",
    "\n",
    "#         self.scene_id = []\n",
    "#         self.scene_map_paths = []\n",
    "        \n",
    "#         # Extract Data:\n",
    "#         self.get_data(data_dir)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "\n",
    "#         # Extract scene map image.\n",
    "#         map_image = Image.open(self.scene_map_paths[idx])\n",
    "#         label_ = np.fromfile(file_name, dtype=np.int32).reshape((-1))\n",
    "#         pointcloud_ = \n",
    "#         assert len(label_) == len(pointcloud_)\n",
    "        \n",
    "#         if self.transform:\n",
    "#             map_image = self.transform(map_image)\n",
    "#         map_tensor = torchvision.transforms.ToTensor()(map_image)\n",
    "#         return map_tensor, pointcloud_tensor\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.scene_id)\n",
    "\n",
    "#     def get_data(self, root_dir):\n",
    "#         sub_directories = os.listdir(root_dir)\n",
    "#         sub_directories.sort()\n",
    "#         for i, sub_directory in enumerate(sub_directories):\n",
    "#             sub_directory = root_dir + sub_directory + '/'\n",
    "#             print(f'Extracting data from [{i}/{len(os.listdir(root_dir))}] directory:  {sub_directory}')\n",
    "#             self.extract_directory(sub_directory)\n",
    "\n",
    "#         print('Extraction Compltete!\\n')\n",
    "\n",
    "#     def data_partition(self, path_list, n):\n",
    "#         chunk_size = len(path_list) // n\n",
    "#         return [[path_list[i:i + chunk_size]] for i in range(0, len(path_list), chunk_size)]\n",
    "\n",
    "#     def extract_directory(self, directory):\n",
    "#         if self.num_workers:\n",
    "#             num_processes = self.num_workers\n",
    "#         else:\n",
    "#             num_processes = mp.cpu_count()\n",
    "\n",
    "#         scene_segments = os.listdir(directory)\n",
    "\n",
    "#         path_list = []\n",
    "#         for scene_segment in scene_segments:\n",
    "#             observation_dir = directory + scene_segment + '/observation'\n",
    "#             observations = os.listdir(observation_dir)\n",
    "#             prediction_dir = directory + scene_segment + '/prediction'\n",
    "#             predictions = os.listdir(prediction_dir)\n",
    "#             print(\"here\")\n",
    "\n",
    "#             assert (len(predictions) == len(observations))\n",
    "\n",
    "#             for observation in observations:\n",
    "#                 path_list.append((directory, scene_segment, observation))\n",
    "\n",
    "#         slices = self.data_partition(path_list, num_processes)\n",
    "#         pool = mp.Pool(processes=num_processes)\n",
    "#         results = pool.starmap(self.extract_submodule_multicore, slices)\n",
    "\n",
    "#         for result in results:\n",
    "#             self.scene_id.extend(result[0])\n",
    "#             self.scene_map_paths.extend(result[1])\n",
    "\n",
    "#     def extract_submodule_multicore(self, path_lists):\n",
    "\n",
    "#         scene_id = []\n",
    "#         future_agent_masks_list = []\n",
    "#         past_agents_state_list = []\n",
    "#         future_agents_state_list = []\n",
    "#         encode_coordinates = []\n",
    "#         decode_coordinates = []\n",
    "#         scene_map_paths = []\n",
    "\n",
    "#         for path_list in path_lists:\n",
    "#             directory, scene_segment, observation = path_list\n",
    "#             observation_path = directory + scene_segment + '/observation/' + observation\n",
    "#             prediction_path = directory + scene_segment + '/prediction/' + observation\n",
    "#             map_path = directory + scene_segment + '/map/v1/' + observation.replace('pkl', 'jpg')\n",
    "\n",
    "#             scene_id.append(scene_segment + '/' + observation)\n",
    "#             scene_map_paths.append(map_path)\n",
    "\n",
    "#         return scene_id, scene_map_paths\n",
    "\n",
    "#     def argoverse_collate(self, batch):\n",
    "#         batch_size = len(batch)\n",
    "#         batch_scene = torch.empty((0))\n",
    "\n",
    "#         for k in range(batch_size):\n",
    "#             # Scene Images\n",
    "#             scene_image = (batch[k][0]).unsqueeze(0)\n",
    "#             batch_scene = torch.cat((batch_scene, scene_image), dim=0)\n",
    "#         return (batch_scene)\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "class JointDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root,  transform=None, num_workers=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        :param data : List of [scene_id, scene_image, number_agents, past_list, future_list,\n",
    "                               encode_coordinates, decode_coordinates]\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.scene_id = []\n",
    "        self.scene_map_paths = []\n",
    "        self.label_files = []\n",
    "        self.pc_files = []\n",
    "        self.root = root\n",
    "        \n",
    "        self.scenes = np.sort(glob.glob(root+\"/*\"))\n",
    "        for each in self.scenes:\n",
    "            self.scene_map_paths.append(np.sort(glob.glob(each+\"/map/*\")))\n",
    "#             print(self.scene_map_paths)\n",
    "            self.label_files.append(np.sort(glob.glob(each+\"/label/*\")))\n",
    "#             print(self.label_files)\n",
    "            self.pc_files.append(np.sort(glob.glob(each+\"/lidar/*\")))\n",
    "#             print(self.label_files)\n",
    "        \n",
    "        self.current_scene = 0\n",
    "        \n",
    "        # Extract Data:\n",
    "#         self.get_data(data_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur = self.current_scene\n",
    "#         self.scene_map_paths = np.array(self.scene_map_paths).T.\n",
    "#         print(self.scene_map_paths[idx])\n",
    "        map_image = Image.open(self.scene_map_paths[cur][idx])\n",
    "        label_ = np.fromfile(self.label_files[cur][idx], dtype=np.int32).reshape((-1))\n",
    "        pointcloud_ply = PyntCloud.from_file(self.pc_files[cur][idx])\n",
    "        pointcloud_ = pointcloud_ply.xyz\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            map_image = self.transform(map_image)\n",
    "        map_tensor = torchvision.transforms.ToTensor()(map_image)\n",
    "        print(map_tensor.shape)\n",
    "        print(pointcloud_.shape)\n",
    "        print(label_.shape)\n",
    "#         assert len(label_) == len(pointcloud_)\n",
    "        print(\"map \", map_tensor.shape)\n",
    "        print(\"pc \", pointcloud_.shape)\n",
    "        return map_tensor, pointcloud_\n",
    "\n",
    "    def __len__(self):\n",
    "        print(len(self.label_files[self.current_scene]))\n",
    "        return len(self.label_files[self.current_scene])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import glob\n",
    "\n",
    "from pyntcloud import PyntCloud\n",
    "\n",
    "class JointDataset2(Dataset):\n",
    "    def __init__(self, root, transform=None, num_workers=None):\n",
    "        self.transform = transform\n",
    "        self.num_workers = num_workers\n",
    "        self.scene_map_paths = []\n",
    "        self.label_files = []\n",
    "        self.pc_files = []\n",
    "        self.root = root\n",
    "        \n",
    "        self.scenes = np.sort(glob.glob(root+\"/*\"))\n",
    "        for each in self.scenes:\n",
    "            self.scene_map_paths.append(np.sort(glob.glob(each+\"/map/*\")))\n",
    "#             print(self.scene_map_paths)\n",
    "            self.label_files.append(np.sort(glob.glob(each+\"/label/*\")))\n",
    "#             print(self.label_files)\n",
    "            self.pc_files.append(np.sort(glob.glob(each+\"/lidar/*\")))\n",
    "#             print(self.label_files)\n",
    "        \n",
    "        self.current_scene = 0\n",
    "    \n",
    "    def get_pc_from_path(self, index):\n",
    "        # Implement your target extraction here\n",
    "        pointcloud_ply = PyntCloud.from_file(self.pc_files[self.current_scene][index])\n",
    "        pointcloud_ = pointcloud_ply.xyz\n",
    "        return torch.tensor(pointcloud_)\n",
    "    \n",
    "    def get_label_from_path(self, index):\n",
    "        # Implement your target extraction here\n",
    "        label_ = np.fromfile(self.label_files[self.current_scene][index], dtype=np.int32).reshape((-1))\n",
    "        return torch.tensor(label_)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        cur = self.current_scene\n",
    "        map_image = Image.open(self.scene_map_paths[cur][index])\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = Image.open(self.scene_map_paths[index])\n",
    "        y = self.get_pc_from_path(index)\n",
    "        z = self.get_label_from_path(index)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        x = torchvision.transforms.ToTensor()(x)\n",
    "        \n",
    "        return x, y, z\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.scene_map_paths[self.current_scene])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\scene1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['data\\\\scene1/map\\\\019.jpg',\n",
       "  'data\\\\scene1/map\\\\020.jpg',\n",
       "  'data\\\\scene1/map\\\\021.jpg',\n",
       "  'data\\\\scene1/map\\\\022.jpg',\n",
       "  'data\\\\scene1/map\\\\023.jpg',\n",
       "  'data\\\\scene1/map\\\\024.jpg',\n",
       "  'data\\\\scene1/map\\\\025.jpg',\n",
       "  'data\\\\scene1/map\\\\026.jpg',\n",
       "  'data\\\\scene1/map\\\\027.jpg',\n",
       "  'data\\\\scene1/map\\\\028.jpg',\n",
       "  'data\\\\scene1/map\\\\029.jpg',\n",
       "  'data\\\\scene1/map\\\\030.jpg',\n",
       "  'data\\\\scene1/map\\\\031.jpg',\n",
       "  'data\\\\scene1/map\\\\032.jpg',\n",
       "  'data\\\\scene1/map\\\\033.jpg',\n",
       "  'data\\\\scene1/map\\\\034.jpg',\n",
       "  'data\\\\scene1/map\\\\035.jpg',\n",
       "  'data\\\\scene1/map\\\\036.jpg',\n",
       "  'data\\\\scene1/map\\\\037.jpg',\n",
       "  'data\\\\scene1/map\\\\038.jpg',\n",
       "  'data\\\\scene1/map\\\\039.jpg',\n",
       "  'data\\\\scene1/map\\\\040.jpg',\n",
       "  'data\\\\scene1/map\\\\041.jpg',\n",
       "  'data\\\\scene1/map\\\\042.jpg',\n",
       "  'data\\\\scene1/map\\\\043.jpg',\n",
       "  'data\\\\scene1/map\\\\044.jpg',\n",
       "  'data\\\\scene1/map\\\\045.jpg',\n",
       "  'data\\\\scene1/map\\\\046.jpg',\n",
       "  'data\\\\scene1/map\\\\047.jpg',\n",
       "  'data\\\\scene1/map\\\\048.jpg',\n",
       "  'data\\\\scene1/map\\\\049.jpg',\n",
       "  'data\\\\scene1/map\\\\050.jpg',\n",
       "  'data\\\\scene1/map\\\\051.jpg',\n",
       "  'data\\\\scene1/map\\\\052.jpg',\n",
       "  'data\\\\scene1/map\\\\053.jpg',\n",
       "  'data\\\\scene1/map\\\\054.jpg',\n",
       "  'data\\\\scene1/map\\\\055.jpg',\n",
       "  'data\\\\scene1/map\\\\056.jpg',\n",
       "  'data\\\\scene1/map\\\\057.jpg',\n",
       "  'data\\\\scene1/map\\\\058.jpg',\n",
       "  'data\\\\scene1/map\\\\059.jpg',\n",
       "  'data\\\\scene1/map\\\\060.jpg',\n",
       "  'data\\\\scene1/map\\\\061.jpg',\n",
       "  'data\\\\scene1/map\\\\062.jpg',\n",
       "  'data\\\\scene1/map\\\\063.jpg',\n",
       "  'data\\\\scene1/map\\\\064.jpg',\n",
       "  'data\\\\scene1/map\\\\065.jpg',\n",
       "  'data\\\\scene1/map\\\\066.jpg',\n",
       "  'data\\\\scene1/map\\\\067.jpg',\n",
       "  'data\\\\scene1/map\\\\068.jpg',\n",
       "  'data\\\\scene1/map\\\\069.jpg',\n",
       "  'data\\\\scene1/map\\\\070.jpg',\n",
       "  'data\\\\scene1/map\\\\071.jpg',\n",
       "  'data\\\\scene1/map\\\\072.jpg',\n",
       "  'data\\\\scene1/map\\\\073.jpg',\n",
       "  'data\\\\scene1/map\\\\074.jpg',\n",
       "  'data\\\\scene1/map\\\\075.jpg',\n",
       "  'data\\\\scene1/map\\\\076.jpg',\n",
       "  'data\\\\scene1/map\\\\077.jpg',\n",
       "  'data\\\\scene1/map\\\\078.jpg',\n",
       "  'data\\\\scene1/map\\\\079.jpg',\n",
       "  'data\\\\scene1/map\\\\080.jpg',\n",
       "  'data\\\\scene1/map\\\\081.jpg',\n",
       "  'data\\\\scene1/map\\\\082.jpg',\n",
       "  'data\\\\scene1/map\\\\083.jpg',\n",
       "  'data\\\\scene1/map\\\\084.jpg',\n",
       "  'data\\\\scene1/map\\\\085.jpg',\n",
       "  'data\\\\scene1/map\\\\086.jpg',\n",
       "  'data\\\\scene1/map\\\\087.jpg',\n",
       "  'data\\\\scene1/map\\\\088.jpg',\n",
       "  'data\\\\scene1/map\\\\089.jpg',\n",
       "  'data\\\\scene1/map\\\\090.jpg',\n",
       "  'data\\\\scene1/map\\\\091.jpg',\n",
       "  'data\\\\scene1/map\\\\092.jpg',\n",
       "  'data\\\\scene1/map\\\\093.jpg',\n",
       "  'data\\\\scene1/map\\\\094.jpg',\n",
       "  'data\\\\scene1/map\\\\095.jpg',\n",
       "  'data\\\\scene1/map\\\\096.jpg',\n",
       "  'data\\\\scene1/map\\\\097.jpg',\n",
       "  'data\\\\scene1/map\\\\098.jpg',\n",
       "  'data\\\\scene1/map\\\\099.jpg',\n",
       "  'data\\\\scene1/map\\\\100.jpg',\n",
       "  'data\\\\scene1/map\\\\101.jpg',\n",
       "  'data\\\\scene1/map\\\\102.jpg',\n",
       "  'data\\\\scene1/map\\\\103.jpg',\n",
       "  'data\\\\scene1/map\\\\104.jpg',\n",
       "  'data\\\\scene1/map\\\\105.jpg',\n",
       "  'data\\\\scene1/map\\\\106.jpg',\n",
       "  'data\\\\scene1/map\\\\107.jpg',\n",
       "  'data\\\\scene1/map\\\\108.jpg',\n",
       "  'data\\\\scene1/map\\\\109.jpg',\n",
       "  'data\\\\scene1/map\\\\110.jpg',\n",
       "  'data\\\\scene1/map\\\\111.jpg',\n",
       "  'data\\\\scene1/map\\\\112.jpg',\n",
       "  'data\\\\scene1/map\\\\113.jpg',\n",
       "  'data\\\\scene1/map\\\\114.jpg',\n",
       "  'data\\\\scene1/map\\\\115.jpg',\n",
       "  'data\\\\scene1/map\\\\116.jpg',\n",
       "  'data\\\\scene1/map\\\\117.jpg',\n",
       "  'data\\\\scene1/map\\\\118.jpg',\n",
       "  'data\\\\scene1/map\\\\119.jpg',\n",
       "  'data\\\\scene1/map\\\\120.jpg',\n",
       "  'data\\\\scene1/map\\\\121.jpg',\n",
       "  'data\\\\scene1/map\\\\122.jpg',\n",
       "  'data\\\\scene1/map\\\\123.jpg',\n",
       "  'data\\\\scene1/map\\\\124.jpg',\n",
       "  'data\\\\scene1/map\\\\125.jpg',\n",
       "  'data\\\\scene1/map\\\\126.jpg']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "scene_map_paths = []\n",
    "# glob.glob(\"data/*\")\n",
    "scenes = glob.glob(\"data\"+\"/*\")\n",
    "for each in scenes:\n",
    "    print(each)\n",
    "    scene_map_paths.append(glob.glob(each+\"/map/*\"))\n",
    "scene_map_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Done\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000277BE6F1588>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch_env\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2637\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2638\u001b[1;33m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2639\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4e2bd6b1fe02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# print(next(iter(trainloader)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;31m# Load labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_pcl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-a02139c71700>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscene_map_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pc_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch_env\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2638\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2639\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2640\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2641\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import pytorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import squeezesegMOD\n",
    "import argparse \n",
    "import datetime\n",
    "import glob \n",
    "\n",
    "\n",
    "### Load settings\n",
    "\t# Dataset path \n",
    "parser = argparse.ArgumentParser(\"./main_train.py\")\n",
    "parser.add_argument(\n",
    "  '--dataset', '-d',\n",
    "  type=str,\n",
    "  default= 'data/',\n",
    "  help='Dataset to train with',\n",
    ")\n",
    "parser.add_argument(\n",
    "  '--arch_cfg', '-ac',\n",
    "  type=str,\n",
    "  default='config/arch/squeezesegV2.yaml',\n",
    "  help='Architecture yaml cfg file. See /config/arch for sample. No default!',\n",
    ")\n",
    "parser.add_argument(\n",
    "  '--data_cfg', '-dc',\n",
    "  type=str,\n",
    "  default='config/labels/argoverse.yaml',\n",
    "  help='Classification yaml cfg file. See /config/labels for sample. No default!',\n",
    ")\n",
    "parser.add_argument(\n",
    "  '--log', '-l',\n",
    "  type=str,\n",
    "#   default='logs/'+ datetime.datetime.now().strftime(\"%Y-%-m-%d-%H:%M\") + '/',\n",
    "  default=\"logs/new/\",\n",
    "  help='Directory to put the log data. Default: ~/logs/date+time'\n",
    ")\n",
    "parser.add_argument(\n",
    "  '--pretrained', '-p',\n",
    "  type=str,\n",
    "  default=None,\n",
    "  help='Directory to get the pretrained model. If not passed, do from scratch!'\n",
    ")\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Opening data config file %s\" % FLAGS.data_cfg)\n",
    "# DATA = yaml.safe_load(open(FLAGS.data_cfg, 'r'))\n",
    "\n",
    "\t# Model parameters\n",
    "\t# Hyper parameters \n",
    "loss = \"xentropy\"       # must be either xentropy or iou\n",
    "max_epochs = 150\n",
    "lr = 0.001              # sgd learning rate\n",
    "wup_epochs = 0.01       # warmup during first XX epochs (can be float)\n",
    "momentum = 0.9          # sgd momentum\n",
    "lr_decay = 0.99         # learning rate decay per epoch after initial cycle (from min lr)\n",
    "w_decay = 0.0001        # weight decay\n",
    "batch_size = 8          # batch size\n",
    "report_batch = 1        # every x batches, report loss\n",
    "report_epoch = 1        # every x epochs, report validation set\n",
    "epsilon_w = 0.001       # class weight w = 1 / (content + epsilon_w)\n",
    "workers = 12            # number of threads to get data\n",
    "dropout = 0.01\n",
    "OS = 16 # output stride (only horizontally)\n",
    "bn_d = 0.01\n",
    "train = True # train backbone?\n",
    "extra = False\n",
    "\n",
    "\n",
    "### Load the data\n",
    "\t# Load the images\n",
    "\t# Load pointcloud\n",
    "train_dataset = JointDataset2(\"data\")\n",
    "print(\"Data Done\")\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "#                                                shuffle=True,\n",
    "                                               num_workers=0,\n",
    "                                               drop_last=True)\n",
    "print(trainloader)\n",
    "# print(next(iter(trainloader)))\n",
    "    # Load labels\n",
    "for batch_num, (data_image, data_pcl, label) in enumerate(trainloader):\n",
    "    print(data_image.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Done\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4ed37380ebf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJointDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data Done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-3207475635ca>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;31m# Extract scene map image.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscene_map_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[0mmap_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscene_map_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mlabel_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_dataset = JointDataset(\"data\")\n",
    "print(\"Data Done\")\n",
    "print(train_dataset.__getitem__(1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
