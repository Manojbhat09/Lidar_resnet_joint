{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from pyntcloud import PyntCloud\n",
    "\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data_pcl_torch, batch_size = 20, max_point_num = 150000, grid_size=1000):\n",
    "    \n",
    "    data_num = np.zeros((batch_size), dtype=np.int32)\n",
    "    indices_split_to_full = np.zeros((batch_size, max_point_num), dtype=np.int32)\n",
    "    \n",
    "    data_pcl = data_pcl_torch.numpy()\n",
    "    print(data_pcl.shape)\n",
    "    \n",
    "    data_pcl_torch = data_pcl_torch.unsqueeze(0)\n",
    "    data_pcl_torch = data_pcl_torch.numpy()\n",
    "    \n",
    "    xyz =data_pcl[:,0:3]\n",
    "    i = data_pcl[:,3]\n",
    "    xyz_min = np.amin(xyz, axis=0, keepdims=True)\n",
    "    print(\"min \", xyz_min)\n",
    "    xyz_max = np.amax(xyz, axis=0, keepdims=True)\n",
    "    print(\"max \", xyz_max)\n",
    "    block_size = (2 * (xyz_max[0, 0] - xyz_min[0, 0]), 2 * (xyz_max[0, 1] - xyz_min[0, 1]) ,  2 * (xyz_max[0, -1] - xyz_min[0, -1]))\n",
    "    \n",
    "    xyz_blocks = np.floor((xyz - xyz_min) / block_size).astype(np.int)\n",
    "    print(\"block size \", block_size)\n",
    "    print(\"diff block \",(xyz - xyz_min))\n",
    "    #print('{}-Collecting points belong to each block...'.format(datetime.now(), xyzrcof.shape[0]))\n",
    "    print(\" blocks \",xyz_blocks)\n",
    "    print(np.unique(xyz_blocks, return_inverse=True,return_counts=True, axis=0))\n",
    "    blocks, point_block_indices, block_point_counts = np.unique(xyz_blocks, return_inverse=True,\n",
    "                                                                return_counts=True, axis=0)\n",
    "    block_point_indices = np.split(np.argsort(point_block_indices), np.cumsum(block_point_counts[:-1]))\n",
    "    #print('{}-{} is split into {} blocks.'.format(datetime.now(), dataset, blocks.shape[0]))\n",
    "\n",
    "    block_to_block_idx_map = dict()\n",
    "    for block_idx in range(blocks.shape[0]):\n",
    "        block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "        block_to_block_idx_map[(block[0], block[1])] = block_idx\n",
    "\n",
    "    # merge small blocks into one of their big neighbors\n",
    "    block_point_count_threshold = max_point_num / 3\n",
    "    print(\"threshold \", block_point_count_threshold)\n",
    "    #print(\"block_point_count_threshold\",block_point_count_threshold)\n",
    "    nbr_block_offsets = [(0, 1), (1, 0), (0, -1), (-1, 0), (-1, 1), (1, 1), (1, -1), (-1, -1)]\n",
    "    block_merge_count = 0\n",
    "    print(\"blocking\")\n",
    "    print(block_point_counts)\n",
    "    print(point_block_indices)\n",
    "    print(blocks)\n",
    "    for block_idx in range(blocks.shape[0]):\n",
    "        if block_point_counts[block_idx] >= block_point_count_threshold:\n",
    "            print(\"here\")\n",
    "            print(block_idx, block_point_counts[block_idx])\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "        for x, y in nbr_block_offsets:\n",
    "            nbr_block = (block[0] + x, block[1] + y)\n",
    "            if nbr_block not in block_to_block_idx_map:\n",
    "                continue\n",
    "\n",
    "            nbr_block_idx = block_to_block_idx_map[nbr_block]\n",
    "            if block_point_counts[nbr_block_idx] < block_point_count_threshold:\n",
    "                continue\n",
    "\n",
    "\n",
    "            #print(block_idx, nbr_block_idx, block_point_counts[nbr_block_idx])\n",
    "\n",
    "            block_point_indices[nbr_block_idx] = np.concatenate(\n",
    "                [block_point_indices[nbr_block_idx], block_point_indices[block_idx]], axis=-1)\n",
    "            block_point_indices[block_idx] = np.array([], dtype=np.int)\n",
    "            block_merge_count = block_merge_count + 1\n",
    "            break\n",
    "    #print('{}-{} of {} blocks are merged.'.format(datetime.now(), block_merge_count, blocks.shape[0]))\n",
    "\n",
    "    idx_last_non_empty_block = 0\n",
    "    for block_idx in reversed(range(blocks.shape[0])):\n",
    "        if block_point_indices[block_idx].shape[0] != 0:\n",
    "            idx_last_non_empty_block = block_idx\n",
    "            break\n",
    "\n",
    "    # uniformly sample each block\n",
    "    for block_idx in range(idx_last_non_empty_block + 1):\n",
    "        point_indices = block_point_indices[block_idx]\n",
    "        if point_indices.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        #print(block_idx, point_indices.shape)\n",
    "        block_points = xyz[point_indices]\n",
    "        block_min = np.amin(block_points, axis=0, keepdims=True)\n",
    "        xyz_grids = np.floor((block_points - block_min) / grid_size).astype(np.int)\n",
    "        grids, point_grid_indices, grid_point_counts = np.unique(xyz_grids, return_inverse=True,\n",
    "                                                                 return_counts=True, axis=0)\n",
    "        grid_point_indices = np.split(np.argsort(point_grid_indices), np.cumsum(grid_point_counts[:-1]))\n",
    "        grid_point_count_avg = int(np.average(grid_point_counts))\n",
    "        point_indices_repeated = []\n",
    "        for grid_idx in range(grids.shape[0]):\n",
    "            point_indices_in_block = grid_point_indices[grid_idx]\n",
    "            repeat_num = math.ceil(grid_point_count_avg / point_indices_in_block.shape[0])\n",
    "            if repeat_num > 1:\n",
    "                point_indices_in_block = np.repeat(point_indices_in_block, repeat_num)\n",
    "                np.random.shuffle(point_indices_in_block)\n",
    "                point_indices_in_block = point_indices_in_block[:grid_point_count_avg]\n",
    "            point_indices_repeated.extend(list(point_indices[point_indices_in_block]))\n",
    "        block_point_indices[block_idx] = np.array(point_indices_repeated)\n",
    "        block_point_counts[block_idx] = len(point_indices_repeated)\n",
    "\n",
    "    idx = 0\n",
    "    for block_idx in range(idx_last_non_empty_block + 1):\n",
    "        point_indices = block_point_indices[block_idx]\n",
    "        if point_indices.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        block_point_num = point_indices.shape[0]\n",
    "        block_split_num = int(math.ceil(block_point_num * 1.0 / max_point_num))\n",
    "        point_num_avg = int(math.ceil(block_point_num * 1.0 / block_split_num))\n",
    "        point_nums = [point_num_avg] * block_split_num\n",
    "        point_nums[-1] = block_point_num - (point_num_avg * (block_split_num - 1))\n",
    "        starts = [0] + list(np.cumsum(point_nums))\n",
    "\n",
    "        np.random.shuffle(point_indices)\n",
    "        block_points = xyz[point_indices]\n",
    "\n",
    "\n",
    "        block_min = np.amin(block_points, axis=0, keepdims=True)\n",
    "        block_max = np.amax(block_points, axis=0, keepdims=True)\n",
    "        #block_center = (block_min + block_max) / 2\n",
    "        #block_center[0][-1] = block_min[0][-1]\n",
    "        #block_points = block_points - block_center  # align to block bottom center\n",
    "        x, y, z = np.split(block_points, (1, 2), axis=-1)\n",
    "        print(i.shape)\n",
    "        print(x.shape)\n",
    "        block_xzyrgbi = np.concatenate([x, z, y, i[:, np.newaxis][point_indices]], axis=-1)\n",
    "        print(block_xzyrgbi.shape)\n",
    "        for block_split_idx in range(block_split_num):\n",
    "            start = starts[block_split_idx]\n",
    "            point_num = point_nums[block_split_idx]\n",
    "            #print(block_split_num, block_split_idx, point_num )\n",
    "\n",
    "\n",
    "\n",
    "            end = start + point_num\n",
    "            idx_in_batch = idx % batch_size\n",
    "            print(idx_in_batch)\n",
    "            data_pcl_torch[idx_in_batch, 0:point_num, ...] = block_xzyrgbi[start:end, :]\n",
    "            data_num[idx_in_batch] = point_num\n",
    "            indices_split_to_full[idx_in_batch, 0:point_num] = point_indices[start:end]\n",
    "\n",
    "            #print(\"indices_split_to_full\", idx_in_batch, point_num, indices_split_to_full)\n",
    "\n",
    "            if  (block_idx == idx_last_non_empty_block and block_split_idx == block_split_num - 1): #Last iteration\n",
    "\n",
    "                item_num = idx_in_batch + 1\n",
    "                \n",
    "            idx = idx + 1\n",
    "    return data_pcl_torch, data_num, indices_split_to_full, item_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file = np.fromfile(os.path.join(\"000000.bin\"),dtype=np.float32).reshape((-1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcl shape  torch.Size([1, 123397, 4])\n",
      "(123397, 4)\n",
      "min  [[-64.347 -79.833  -6.962]]\n",
      "max  [[68.529 77.739  2.897]]\n",
      "block size  (265.75201416015625, 315.14398193359375, 19.718000411987305)\n",
      "[[87.066     79.864      7.939    ]\n",
      " [82.399     79.909      7.782    ]\n",
      " [82.373     79.965004   7.781    ]\n",
      " ...\n",
      " [68.124     78.424      5.198    ]\n",
      " [68.122     78.439      5.2019997]\n",
      " [69.981     78.438      4.3719997]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " ...\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "threshold  50000.0\n",
      "blocking\n",
      "[123397]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[0 0 0]]\n",
      "here\n",
      "0 123397\n",
      "(123397,)\n",
      "(123397, 1)\n",
      "(123397, 4)\n",
      "0\n",
      "(1, 123397, 4)\n",
      "(20,)\n",
      "(20, 150000)\n",
      "1\n",
      "[[[-12.981  -0.57   -4.302   0.55 ]\n",
      "  [-32.398  -0.779  19.336   0.41 ]\n",
      "  [ 12.082  -1.104   3.179   0.57 ]\n",
      "  ...\n",
      "  [  7.381  -1.714  -3.677   0.36 ]\n",
      "  [ -4.387  -1.914   6.079   0.24 ]\n",
      "  [  2.368   0.484   7.765   0.42 ]]]\n",
      "(1, 123397, 4)\n",
      "(1,)\n",
      "(1, 150000)\n",
      "(20, 123397, 4)\n",
      "123397\n"
     ]
    }
   ],
   "source": [
    "main_tensor = np.arange(150000*4).reshape((150000,4))\n",
    "# main_tensor = load_file[:, np.newaxis]\n",
    "main_tensor = load_file\n",
    "batch_size = 20\n",
    "# main_tensor[:full_data.shape[0], :full_data.shape[1]] = full_data\n",
    "data_pcl = torchvision.transforms.ToTensor()(main_tensor)\n",
    "print(\"pcl shape \",data_pcl.shape)\n",
    "for each_batch in data_pcl:\n",
    "    batched_data, data_num, indices_split_to_full, item_num = get_batches(each_batch, batch_size = batch_size)\n",
    "    print(batched_data.shape)\n",
    "    print(data_num.shape)\n",
    "    print(indices_split_to_full.shape)\n",
    "    print(item_num)\n",
    "    print(batched_data)\n",
    "    \n",
    "    data =batched_data[0:item_num, ...].astype(np.float32) \n",
    "    print(data.shape)\n",
    "\n",
    "    data_num =data_num[0:item_num, ...] \n",
    "    print(data_num.shape)\n",
    "    indices_split_to_full = indices_split_to_full[0:item_num]\n",
    "    print(indices_split_to_full.shape)\n",
    "\n",
    "    batch_num = data.shape[0]\n",
    "    for batch_idx in range(batch_num):\n",
    "        points_batch = data[[batch_idx] * batch_size, ...]\n",
    "        print(points_batch.shape)\n",
    "        point_num = data_num[batch_idx]\n",
    "        print(point_num)\n",
    "        break\n",
    "\n",
    "        tile_num = int ( math.ceil((sample_num * batch_size) / point_num) )\n",
    "        indices_shuffle = np.tile(np.arange(point_num), tile_num)[0:sample_num * batch_size]\n",
    "        np.random.shuffle(indices_shuffle)\n",
    "        indices_batch_shuffle = np.reshape(indices_shuffle, (batch_size, sample_num, 1))\n",
    "        indices_batch = np.concatenate((indices_batch_indices, indices_batch_shuffle), axis=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
